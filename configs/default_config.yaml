model:
  model_name: "llama-7b"
  vocab_size: 32000
  hidden_size: 4096
  intermediate_size: 11008
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 32
  max_position_embeddings: 2048
  rms_norm_eps: 1e-6
  rope_theta: 10000.0
  attention_dropout: 0.0

training:
  batch_size: 4
  learning_rate: 1e-4
  num_epochs: 3
  max_seq_length: 512
  warmup_steps: 100
  weight_decay: 0.01
  gradient_clipping: 1.0

galore:
  rank: 1024
  update_proj_gap: 500
  scale: 0.25
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

federated:
  num_clients: 3
  num_rounds: 10
  client_fraction: 1.0
  local_epochs: 1
  server_address: "localhost"
  server_port: 8080

knowledge_distillation:
  temperature: 3.0
  alpha: 0.5
  
